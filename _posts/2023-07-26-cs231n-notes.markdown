---
layout: post
title: 'Notes for CS231n'
date: '2023-07-26 22:00:00'
---

## Image Classification 

- challenges: scale, deformation, occulsion, lighting, background

data-driven approach >> hardcoding an algorithm to recognizing an object

### image classification pipeline

> input -> learning -> evaluation

### nearest neighbor classifier

L1 distance -> 38.6% accuracy
L2 distance -> 35.4% accuracy

![L1](/assets/L1.jpg)

L1 vs L2 - L2 is more tighter in scoring the difference between two vectors. special subset of p-norm


#### k-nearest neighbor classifier

instead of finding the single closest image in the training set, we will find the top k closest images, and have them vote on the label of the test image. 

![knn](/assets/knn.jpg)

k=1 is essentially the basic "nearest neighbor classifier". 

k is a **hyperparameter**. 

*hyperparameters* should be configured through cross-validation (i.e. picking an arbitrary small percentage of the training data to tune which hyperparameters work best)

![folds](/assets/folds.jpg)

5-fold cross-validation - 4 for training 1 for validation

more hyperparameters bigger validation splits

cons - cheap to train but expensive to test

useful when data is low dimensional. 

### Linear Classification

score function -> maps raw data to class scores

loss function -> quantifies the agreement between the predicted scores and ground truth labels.

### score function

maps raw image pixels to class scores

![linear classifier](/assets/linearClassifier.jpg)

Interpreting it

![interptered linear classifier](/assets/linearClassifierInterpreted.jpg)

as template matching

![template matching](/assets/templateMatching.jpg)

#### *bias trick* 

![bias trick](/assets/biasTrick.jpg)

### loss function

also known as cost function or objective. loss high, bad job. loss low, good job

#### Multiclass Support Vector Machines (SVM)

**SVM** *wants* the correct class for each image to have a score higher than the incorrect
classes by a fixed **hyperparameter**

![SVM vs Softmax](/assets/svmsoftmax.jpg)

### Optimization

three main components to neural networks. ***Score***, ***Loss***, ***Optimization***. 
The goal of **optimization** is to find W(eights) such that minimize the *loss function*.

- **random search** -> terrible 
- **random local search** -> take the core idea from random search. "iterative refinement" 
- **gradients** -> they tell us the slope of the loss function along every dimension

the **gradients** tell us the direction whihch has the steepest rate of increase, but does not tell us
how far alone this direction to step. another *hyperparameter* known as **learning rate**. 

the algorithm to iteracively compute the gradient and perform a paremater update in the loop
is known as **gradient descent**

### Backpropogation

The core problem: given a function **f(x)** where x is a vector of inputs and we are interested
in computing *grad f(x)*


